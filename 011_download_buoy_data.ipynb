{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download buoy data\n",
    "\n",
    "We'll download all realtime buoy data from <ftp://ftp.pmel.noaa.gov/high_resolution/realtime/cdf/> (need to be logged in to see this directory).\n",
    "\n",
    "For now, we'll get:\n",
    "- temperature `t[0-9]*hr.cdf*`\n",
    "- currents `cur*hr.cdf*`\n",
    "- winds `w[0-9]*hr.cdf*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "buoy_file_name = \"tmp_buoy_data\"  # ....nc / ....csv\n",
    "raw_data_dir = \"raw_buoy_data\"\n",
    "buoy_positions_file = \"tmp_buoy_positions.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tech preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hvplot.pandas, hvplot.xarray\n",
    "from pathlib import Path\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget_call = (\n",
    "    'wget -e robots=off -nv -r -c -np -nH --cut-dirs=3'\n",
    "    ' --user=\"$TAO_FTP_USER\" --password=\"$TAO_FTP_PASS\"'\n",
    "    ' --accept \"t[0-9]*hr.cdf*,cur*hr.cdf*,w[0-9]*hr.cdf*\"'\n",
    "    f' -P \"{raw_data_dir}\"'\n",
    "    ' \"ftp://ftp.pmel.noaa.gov/high_resolution/realtime/cdf/\"'\n",
    ")\n",
    "wget_call_w_cred_sourcing = f\"'source .ftp_cred && {wget_call}'\"\n",
    "wget_call = f\"'{wget_call}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rfv {raw_data_dir}\n",
    "!mkdir -p {raw_data_dir}\n",
    "# try download with credentials from the env vars:\n",
    "!bash -c {wget_call}\n",
    "# if this fails, sourcing the credentials from a file may work:\n",
    "!bash -c {wget_call_w_cred_sourcing}\n",
    "!gunzip -v {raw_data_dir}/*.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all data files and load separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = list(sorted(Path(raw_data_dir).glob(\"*hr.cdf\")))\n",
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Dask backend, because without, merging seems slow?\n",
    "data_sets = {\n",
    "    p.name: xr.open_dataset(p, chunks={}) for p in data_files\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge into one xarray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = reduce(lambda d0, d1: xr.merge((d0, d1)), data_sets.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(f\"{buoy_file_name}.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncdump -h {buoy_file_name}.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast to Pandas dataframes and save as one CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [ds.to_dataframe().reset_index() for ds in data_sets.values()]\n",
    "df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{buoy_file_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n10 {buoy_file_name}.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract buoy positions and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buoy_positions = df.groupby([\"lat\", \"lon\"]).size().reset_index().drop(columns=0)\n",
    "buoy_positions = buoy_positions.sort_values([\"lat\", \"lon\"], ascending=False)\n",
    "buoy_positions = buoy_positions.reset_index(drop=True)\n",
    "buoy_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buoy_positions.to_csv(buoy_positions_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some sanity check: Plot time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{buoy_file_name}.csv\")\n",
    "df = df.set_index([\"lat\", \"lon\", \"depth\"])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lon, depth = df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.loc[(lat, lon, depth)].hvplot.line(\"time\", \"U_320\")\n",
    "    * df.loc[(lat, lon, depth)].hvplot.line(\"time\", \"V_321\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(f\"{buoy_file_name}.nc\")\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ds.sel(lat=lat, lon=lon, depth=depth, drop=True)[\"U_320\"].hvplot.line()\n",
    "    * ds.sel(lat=lat, lon=lon, depth=depth, drop=True)[\"V_321\"].hvplot.line()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"Finished: $(date -Ins) UTC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "See https://github.com/willirath/nia-prediction-low-latitudes for details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
